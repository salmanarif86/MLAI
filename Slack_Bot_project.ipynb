{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Slack Bot project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salmanarif86/MLAI/blob/master/Slack_Bot_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7tONKA7t2K0",
        "colab_type": "text"
      },
      "source": [
        "**Problem statement:**\n",
        "\n",
        "I need a way to more quickly understand what's happening in the Kaggle forums and act on it. I want a faster way to summerize trends on forums posts, figure out what questions are good for me to answer and alert my teammates if the community is reporting something en masse.\n",
        "\n",
        "This is actually three seperate problems:\n",
        "\n",
        "Summerization (level of activicty, new/emerging topics, topics that are newly popular)\n",
        "Flag questions I'm likely to know the answer to\n",
        "Identify possible answerers for a given question\n",
        "Alerts based on anomaly detection (lots of community discussion around a specific topic)\n",
        "\n",
        "**Measuring success:**\n",
        "\n",
        "*Summerization:*\n",
        "\n",
        "       * User feedback on bot output (online learning)\n",
        "       * Usupervised NLU\n",
        "       \n",
        "              * Manual verification of topics\n",
        "              * Manual verification of keywords\n",
        "              \n",
        "       *Flagging questions:\n",
        "       \n",
        "              * Accuracy of predicting questions I replied using my forum history\n",
        "              \n",
        "       *Alerts\n",
        "          \n",
        "              * Accuracy of past event/bugs\n",
        "\n",
        "*Possible approches to summerization:*\n",
        "\n",
        "  **Level of activicty**\n",
        "  \n",
        "        * time series modelling of # of posts over time\n",
        "        * X posts this week (+- from last week), most popular (upvotes), most replied to\n",
        "        \n",
        "** Keywords**\n",
        "\n",
        "      * https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf\n",
        "      * Faster\n",
        "\n",
        "**Topics**\n",
        "\n",
        "    * More flexible to differences in vocabulary\n",
        "    \n",
        "**Hybrid Approach**\n",
        "\n",
        "  * Keywords + embeddings to group similar keywords\n",
        "  \n",
        "**Clustering based on embeddings**\n",
        "\n",
        "  * Do we want to train our own embeddings?\n",
        "  * Look into current approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMqTzszyenmg",
        "colab_type": "text"
      },
      "source": [
        "**Notes**\n",
        "\n",
        "\n",
        "**Unsupervised text Clustering**\n",
        "\n",
        "*Words - > Inputs*\n",
        "\n",
        "* Embeddings (might not be helpful if we dont train new embeddings for each time we run the model)\n",
        "\n",
        "    * Fasttext can handle out of vocabulary words\n",
        "    * Subword embeddings\n",
        "    * Biggest factor - How long do they take to train?\n",
        "    * Universal Sentence Encoder Embedding - We train our own embedding each time\n",
        "\n",
        "* Td-idf\n",
        "\n",
        "* LDA\n",
        "\n",
        "* Take the frequency matrix, remove the expected frequency (by subtracting, or using the column marginal as a noise model) -Leland McInnes\n",
        "\n",
        "* Embedding weighted with tf-idf\n",
        "\n",
        "* Embedding and then performing PCA, removing first principal component - Arora (2018) - 'A simple but tough to beat baseline sentence embeddings'\n",
        "\n",
        "* pLSA (Cheaper version of LDA)\n",
        "\n",
        "\n",
        "*Topic Modelling*\n",
        "\n",
        "* LDA\n",
        "        \n",
        "        *Too slow for this particular use case\n",
        "        *Hard to interpret\n",
        "        \n",
        "\n",
        "*Clustering with Embeddings*\n",
        "\n",
        "* Hierach. Clustering\n",
        "* Brown Clusters\n",
        "\n",
        "        * Hierachical\n",
        "        * work on the word level\n",
        "        * can be updated actively\n",
        "        * would need to find a python code\n",
        "        \n",
        "* DBSCAN\n",
        "\n",
        "      * needs embedding as input\n",
        "      * should reduce dimensionality\n",
        "      * note: clusters should be of similar density\n",
        "      * HDSCAN is the hierachical version\n",
        "\n",
        "--------------------------------------------------------\n",
        "\n",
        "\n",
        "Whole pipeline : \n",
        "\n",
        "* https://topsmb.github.io\n",
        "* https://github.com/bigartm/bigartm\n",
        "\n",
        "1. Words to  numbers\n",
        "\n",
        "* tf-idf\n",
        "* LDA\n",
        "* pLSA\n",
        "* Embeddings\n",
        "\n",
        "    * fasttext\n",
        "    * USE (universal sentence encoder embeddings ) Embeddings\n",
        "    * GloVe?\n",
        "    * Word2Vec\n",
        "    * ELMo Embeddings\n",
        "    * text to knowledge mapping - mapping text to knowledge using Multi-Sense LSTMs - wont probably do as this would require building our own knoeldge graph -   no bandwith\n",
        "\n",
        "2. Dimensionality Reduction\n",
        "  \n",
        "    * UMAP\n",
        "    * PCA\n",
        "    \n",
        "3. Clustering\n",
        "\n",
        "    * DBSCAN\n",
        "    * HDBSCAN\n",
        "    * Specteral Clusterings\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTPYfy5w5Raj",
        "colab_type": "text"
      },
      "source": [
        "Experimenting with YAKE!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmRadv4XehhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install git+https://github.com/LIAAD/yake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWFIPAqH4f9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import yake\n",
        "import pandas as pd\n",
        "from Clustering import Corpus\n",
        "from Brown_Clustering_yangyuan import *\n",
        "from nltk import word_tokenize\n",
        "from nltk import RegexpTokenizer\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYeTXSiXJ9OC",
        "colab_type": "text"
      },
      "source": [
        "#### Testing the Yake algorithm with a custom sentence to get a feel of how the algorthim is doing interms of extracting keywords. This method takes in custom pre-defined parameters where we dont have to tweek anything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpADd8RgAVuJ",
        "colab_type": "code",
        "outputId": "5baf49f4-6e65-4246-8274-897e0d0bb09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "text_content = \"\"\"\n",
        "\tSources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\n",
        "\tcompetitions. Details about the transaction remain somewhat vague , but given that Google is hosting\n",
        "\tits Cloud Next conference in San Francisco this week, the official announcement could come as early\n",
        "\tas tomorrow.  Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the\n",
        "\tacquisition is happening. Google itself declined 'to comment on rumors'.\n",
        "\"\"\"\n",
        "\n",
        "# assuming default parameters\n",
        "simple_kwextractor = yake.KeywordExtractor()\n",
        "keywords = simple_kwextractor.extract_keywords(text_content)\n",
        "\n",
        "for kw in keywords:\n",
        "\tprint(kw)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('machine learning competitions', 0.005240218636588412)\n",
            "('hosts data science', 0.007231800172852763)\n",
            "('learning competitions', 0.02651298908496934)\n",
            "('platform that hosts', 0.03633877348319309)\n",
            "('hosts data', 0.03633877348319309)\n",
            "('data science', 0.03633877348319309)\n",
            "('science and machine', 0.03633877348319309)\n",
            "('machine learning', 0.03633877348319309)\n",
            "('ceo anthony goldbloom', 0.03727546242790534)\n",
            "('acquiring kaggle', 0.046501713202057565)\n",
            "('kaggle co-founder ceo', 0.05500284979172434)\n",
            "('san francisco', 0.05743727907793731)\n",
            "('google', 0.06726505100386607)\n",
            "('google is acquiring', 0.06754045633911093)\n",
            "('anthony goldbloom declined', 0.07472471161713069)\n",
            "('co-founder ceo anthony', 0.07489379267114575)\n",
            "('francisco this week', 0.09080847547468633)\n",
            "('ceo anthony', 0.10396234776227407)\n",
            "('anthony goldbloom', 0.10396234776227407)\n",
            "('hosting its cloud', 0.11556884354166654)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVcErzrHKXcE",
        "colab_type": "text"
      },
      "source": [
        "#### Same algorthim on the same corpus but tis method allows us to input some parameters in order to tweek it based on our use case. Looks like the default parameters give almost the same results as the custom one. So we are just going to use simple keyword extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT3zdIXn59Rk",
        "colab_type": "code",
        "outputId": "11b1db2d-693c-4d4e-b371-a3ac0c18c838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# specifying parameters\n",
        "custom_kwextractor = yake.KeywordExtractor(lan=\"en\", n=3, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n",
        "\n",
        "keywords = custom_kwextractor.extract_keywords(text_content)\n",
        "\n",
        "for kw in keywords:\n",
        "\tprint(kw)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('machine learning competitions', 0.005240218636588412)\n",
            "('hosts data science', 0.007231800172852763)\n",
            "('learning competitions', 0.02651298908496934)\n",
            "('platform that hosts', 0.03633877348319309)\n",
            "('hosts data', 0.03633877348319309)\n",
            "('data science', 0.03633877348319309)\n",
            "('science and machine', 0.03633877348319309)\n",
            "('machine learning', 0.03633877348319309)\n",
            "('ceo anthony goldbloom', 0.03727546242790534)\n",
            "('acquiring kaggle', 0.046501713202057565)\n",
            "('kaggle co-founder ceo', 0.05500284979172434)\n",
            "('san francisco', 0.05743727907793731)\n",
            "('google', 0.06726505100386607)\n",
            "('google is acquiring', 0.06754045633911093)\n",
            "('anthony goldbloom declined', 0.07472471161713069)\n",
            "('co-founder ceo anthony', 0.07489379267114575)\n",
            "('francisco this week', 0.09080847547468633)\n",
            "('ceo anthony', 0.10396234776227407)\n",
            "('anthony goldbloom', 0.10396234776227407)\n",
            "('hosting its cloud', 0.11556884354166654)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YviQyGZ7ME_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forum_post = pd.read_csv('/content/ForumMessages.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaAWIlnjK7TH",
        "colab_type": "text"
      },
      "source": [
        "### Now trying the simple keyword extractor on our forum text  and we will try a few meesage post to see the results and get some intuttion into the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8ClVfX7BDJL",
        "colab_type": "code",
        "outputId": "c7f113af-737e-467f-e9c9-f459b7499613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "simple_kwextractor.extract_keywords(forum_post.Message[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('free hunch blog', 0.00036928641235685713),\n",
              " ('div', 0.003962474153257461),\n",
              " ('hunch blog', 0.0050494898992956925),\n",
              " ('free hunch', 0.005274063752072036),\n",
              " ('href', 0.026476296695875336),\n",
              " ('features', 0.05842835043495728),\n",
              " ('blog', 0.06936327841616126),\n",
              " ('free', 0.07243199525061537),\n",
              " ('hunch', 0.07243199525061537),\n",
              " ('mce', 0.0803814044228403),\n",
              " ('train a svm', 0.14354624602359264),\n",
              " ('svm', 0.2056295305809825),\n",
              " ('sequences', 0.21175280747641545),\n",
              " ('sequence', 0.21175280747641545),\n",
              " ('los alamos', 0.23088984029260495),\n",
              " ('website', 0.23849729378219245),\n",
              " ('give', 0.2394444994311782),\n",
              " ('response', 0.2524750771695876),\n",
              " ('comment', 0.2524750771695876),\n",
              " ('model', 0.2722507351479002)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhvGiOR-ByAH",
        "colab_type": "code",
        "outputId": "767dd7b4-72a3-4a0d-f01b-c2b11584b064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "simple_kwextractor.extract_keywords(forum_post.Message[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('super computing power', 0.0007587825802569232),\n",
              " ('released upon entry.', 0.007789229896782814),\n",
              " ('computing power', 0.007789229896782814),\n",
              " ('interested in participating', 0.009281830427428734),\n",
              " ('access to proprietary', 0.009281830427428734),\n",
              " ('proprietary software', 0.009281830427428734),\n",
              " ('software and super', 0.009281830427428734),\n",
              " ('super computing', 0.009281830427428734),\n",
              " ('predictions work', 0.009281830427428734),\n",
              " ('simulated by users', 0.0670458894265181),\n",
              " ('trademarked and licensed.', 0.0670458894265181),\n",
              " ('platform', 0.07536829389396676),\n",
              " ('possibly even release', 0.07813908785389614),\n",
              " ('release a model', 0.07813908785389614),\n",
              " ('make a contribution', 0.07813908785389614),\n",
              " ('contest', 0.08059613614664227),\n",
              " ('entry.', 0.08059613614664227),\n",
              " ('power', 0.08059613614664227),\n",
              " ('interested', 0.09589822912399033),\n",
              " ('participating', 0.09589822912399033)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2PGYuqvCpAf",
        "colab_type": "text"
      },
      "source": [
        "####  Now we are going to take keywords for each post and turn them into a text string \"sentence\". This \"sentence\" will serve as an input into our brown cluster algorthim\n",
        "\n",
        "#### Lower the score of our clusters higher the importance\n",
        "\n",
        "#### We will first try it on the first 100 messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3TH-jk4G-bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_messages = forum_post.Message[-1000:].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYwvOxSLCocY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "6e377eef-f992-46ea-f7a8-b0689500e599"
      },
      "source": [
        "sentences =[]\n",
        "\n",
        "for post in sample_messages:\n",
        "  \n",
        "  \n",
        "  post_keywords = simple_kwextractor.extract_keywords(post)\n",
        "  \n",
        "  \n",
        "  sentences_output = ''\n",
        "  \n",
        "  for words, numbers in post_keywords:\n",
        "    \n",
        "    sentences_output = sentences_output + words + \" \"\n",
        "    \n",
        "  \n",
        "  sentences.append(sentences_output)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/yake/datarepresentation.py:106: RuntimeWarning: Mean of empty slice.\n",
            "  avgTF = validTFs.mean()\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
            "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqrXfvvM_Bg",
        "colab_type": "text"
      },
      "source": [
        "#### This will tokenize our \"sentences\" as well as remove any punctutions. This is a nltk method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWK_nUZ_FGhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "sample_data_tokenized =[w.lower() for w in sentences]\n",
        "sample_data_tokenized =[tokenizer.tokenize(i) for i in sample_data_tokenized]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck0FAgMbNTEJ",
        "colab_type": "text"
      },
      "source": [
        "#### Now we will use the brown clustering algorthim and put it to training. This should take less time to run as its just runing on keywords as oppose to runing on Messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFoEYZPEUtPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(sample_data_tokenized, 0.001)\n",
        "clustering = BrownClustering(corpus, 6)\n",
        "clustering.train()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj44CSiiObND",
        "colab_type": "text"
      },
      "source": [
        "#### Now lets test out and see what similar words to we get after the algorthim has been trained on our keyword Corpus. Now by checking whats similar to submission. Now i think the numbers within each tuple is mutual information.High mutual information indicates a large reduction in uncertainty; low mutual information indicates a small reduction; and zero mutual information between two random variables means the variables are independent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI7D8MdEOtC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "eba76696-b3b9-48da-ccb2-b3f1c63b89eb"
      },
      "source": [
        "clustering.get_similar('submission')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('data', 385),\n",
              " ('kaggle', 385),\n",
              " ('dataset', 385),\n",
              " ('set', 385),\n",
              " ('test', 385),\n",
              " ('train', 385),\n",
              " ('model', 385),\n",
              " ('make', 385),\n",
              " ('public', 385),\n",
              " ('team', 385)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIXplbu8WcPB",
        "colab_type": "text"
      },
      "source": [
        "#### Now lets see if the word \"data\" would give us the same words. Looks like it doent and my guess would be that brown cluster algorthim probably returns words with the highest reduction in uncertanity but within the same cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIJinj3HVSPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "05a18a6d-bde7-4fb1-80ef-d2dc3dd51390"
      },
      "source": [
        "clustering.get_similar('data')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model', 400),\n",
              " ('kaggle', 399),\n",
              " ('test', 398),\n",
              " ('set', 397),\n",
              " ('private', 396),\n",
              " ('public', 395),\n",
              " ('dataset', 394),\n",
              " ('make', 393),\n",
              " ('train', 392),\n",
              " ('file', 391)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4q4PUEiXOPf",
        "colab_type": "text"
      },
      "source": [
        "#### The results look decent but I am not sure if this will give us cohernt clusters which we as humans can look at and infer the topic. I think the keyword extraction/ dimensionality reduction and then clustering worked really well. Usually keyword extraction reduces the informativness of the n-gram but we didnt see that problem happening. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoCvJRIYWzAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}